{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bec7704",
   "metadata": {},
   "source": [
    "Dataflow Gen2 is suitable for static ingestion, but since we are designing a configuration-driven dynamic RDM framework with logic branching, a notebook provides the required programmability and scalability.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341eda7",
   "metadata": {},
   "source": [
    "üîπ when we use Notebook ?\n",
    "\n",
    "whrn:\n",
    "\n",
    "‚úî Dynamic behaviour \n",
    "‚úî Config read and then logic change\n",
    "‚úî For loop \n",
    "‚úî Conditional logic apply \n",
    "‚úî Conformed vs Mapping branch \n",
    "‚úî _add detection build \n",
    "\n",
    "Notebook is programmable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4758332",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "fonfig_df == read that scv uploaded in filves  (overite at uploding time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: We already loaded the config CSV into config_df.\n",
    "# Now we will loop through each config row and sanity-check what we will ingest.\n",
    "\n",
    "def print_config_summary(cfg):\n",
    "    \"\"\"\n",
    "    Print a readable summary of one config row.\n",
    "    This is just a verification step before we start reading SharePoint lists dynamically.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Config Name     : {cfg['rdm_config_name']}\")\n",
    "    print(f\"Entity          : {cfg['entity_name']}\")\n",
    "    print(f\"SharePoint List : {cfg['sp_list_name']}\")\n",
    "    print(f\"Target Table    : {cfg['silver_table_name']}\")\n",
    "    print(f\"RDM Type        : {cfg['rdm_type']}\")\n",
    "    print(f\"Include Columns : {cfg['include_columns']}\")\n",
    "    print(f\"Business Keys   : {cfg['business_key_columns']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# Collect config rows from the Spark DataFrame\n",
    "config_rows = config_df.collect()\n",
    "\n",
    "# Print a summary for each row so we can confirm the config looks correct\n",
    "for cfg in config_rows:\n",
    "    print_config_summary(cfg.asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993b162",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare config rows for dynamic processing\n",
    "# Convert comma-separated strings into Python lists so we can use them safely later.\n",
    "\n",
    "prepared_configs = []\n",
    "\n",
    "for cfg in config_df.collect():\n",
    "    cfg_dict = cfg.asDict()\n",
    "\n",
    "    include_cols = [c.strip() for c in cfg_dict[\"include_columns\"].split(\",\") if c and c.strip()]\n",
    "    business_keys = [c.strip() for c in cfg_dict[\"business_key_columns\"].split(\",\") if c and c.strip()]\n",
    "\n",
    "    prepared_configs.append({\n",
    "        \"config_name\": cfg_dict[\"rdm_config_name\"],\n",
    "        \"entity_name\": cfg_dict[\"entity_name\"],\n",
    "        \"sp_list_name\": cfg_dict[\"sp_list_name\"],\n",
    "        \"target_table\": cfg_dict[\"silver_table_name\"],\n",
    "        \"rdm_type\": cfg_dict[\"rdm_type\"],\n",
    "        \"include_columns\": include_cols,\n",
    "        \"business_keys\": business_keys\n",
    "    })\n",
    "\n",
    "# Human-readable check\n",
    "for pc in prepared_configs:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"Config Name   : {pc['config_name']}\")\n",
    "    print(f\"Entity        : {pc['entity_name']}\")\n",
    "    print(f\"SP List       : {pc['sp_list_name']}\")\n",
    "    print(f\"Target Table  : {pc['target_table']}\")\n",
    "    print(f\"RDM Type      : {pc['rdm_type']}\")\n",
    "    print(f\"Include Cols  : {pc['include_columns']}\")\n",
    "    print(f\"Business Keys : {pc['business_keys']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3f06e",
   "metadata": {},
   "source": [
    "1 sharepoint list read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "-------------------old------------\n",
    "\n",
    "# Step 2: Read each mock SharePoint list export (CSV) based on config\n",
    "# For POC, we're reading exported SharePoint lists from Lakehouse Files.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "base_path = \"Files/mock_rdm_lists\"\n",
    "\n",
    "for cfg in prepared_configs:\n",
    "    list_name = cfg[\"sp_list_name\"]\n",
    "    include_cols = cfg[\"include_columns\"]\n",
    "    target_table = cfg[\"target_table\"]\n",
    "\n",
    "    # Build the file path for the exported list\n",
    "    file_path = f\"{base_path}/{list_name}.csv\"\n",
    "\n",
    "    print(f\"\\nReading list export: {file_path}\")\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(file_path)\n",
    "    )\n",
    "\n",
    "    # Select only the columns we want (ignore everything else in the SharePoint list)\n",
    "    df_selected = df.select([F.col(c) for c in include_cols])\n",
    "\n",
    "    print(f\"Writing to target table: {target_table}\")\n",
    "    \n",
    "    # For POC we can overwrite; for production we would avoid overwrite and use a safer pattern\n",
    "    df_selected.write.mode(\"overwrite\").format(\"delta\").saveAsTable(target_table)\n",
    "\n",
    "    print(f\"Done: {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc1463",
   "metadata": {},
   "source": [
    "updated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Clean Professional Ingestion Loop (POC)\n",
    "# ------------------------------------------------------------\n",
    "# This loop dynamically reads SharePoint-exported CSV files\n",
    "# based on config and writes them into Silver Delta tables.\n",
    "#\n",
    "# Original Config Fields:\n",
    "# - sp_list_name      ‚Üí tells us which SharePoint list export to read\n",
    "# - include_columns   ‚Üí tells us which columns to keep\n",
    "# - silver_table_name ‚Üí tells us where to write in Silver\n",
    "#\n",
    "# Additional Config Fields (Added for future scalability):\n",
    "#\n",
    "# - rdm_type:\n",
    "#     Identifies whether the RDM is CONFORMED or MAPPING.\n",
    "#     This allows future branching logic (e.g., different validation,\n",
    "#     transformation or merge rules per RDM type).\n",
    "#\n",
    "# - business_keys:\n",
    "#     Defines the natural key of the dataset.\n",
    "#     These keys will be used in production for:\n",
    "#         * duplicate detection\n",
    "#         * null validation\n",
    "#         * MERGE / UPSERT logic instead of overwrite\n",
    "#     NOTE: rdm_type and business_keys are NOT written into the Silver table.\n",
    "#           They are only used to control ingestion logic.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "base_path = \"Files/mock_rdm_lists\"\n",
    "\n",
    "for cfg in prepared_configs:\n",
    "\n",
    "    list_name = cfg[\"sp_list_name\"]\n",
    "    include_cols = cfg[\"include_columns\"]\n",
    "    target_table = cfg[\"target_table\"]\n",
    "\n",
    "    # Newly added metadata fields\n",
    "    rdm_type = cfg[\"rdm_type\"]\n",
    "    business_keys = cfg[\"business_keys\"]\n",
    "\n",
    "    file_path = f\"{base_path}/{list_name}.csv\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Processing config   : {cfg['config_name']}\")\n",
    "    print(f\"RDM Type            : {rdm_type}\")\n",
    "    print(f\"Target Silver table : {target_table}\")\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(file_path)\n",
    "    )\n",
    "\n",
    "    # Keep only configured columns\n",
    "    df_selected = df.select([F.col(c) for c in include_cols])\n",
    "\n",
    "    # Basic validation on business keys (for future production readiness)\n",
    "    for key in business_keys:\n",
    "        null_count = df_selected.filter(F.col(key).isNull()).count()\n",
    "        print(f\"Null count in business key '{key}': {null_count}\")\n",
    "\n",
    "    # POC behavior: overwrite\n",
    "    df_selected.write.mode(\"overwrite\").format(\"delta\").saveAsTable(target_table)\n",
    "\n",
    "    print(f\"Finished writing table : {target_table}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
